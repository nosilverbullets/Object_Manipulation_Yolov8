{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPxdArH547mt"
      },
      "source": [
        "Trabajo práctico II\n",
        "\n",
        "## Detección de objetos con Deep Learning, CNN y YOLO\n",
        "\n",
        "Braian D'Aleo - Lic. en Ciencia de Datos - UGR 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para este trabajo, preferi usar el entorno local para acelerar los procesos. En colab procesar un video de 20 segundos me demora unos 5 minutos, usando mi entorno local puedo hacer lo mismo en 1 minuto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w_9ONrz4mjs",
        "outputId": "aa04977c-2238-4137-b9e9-dea314a59a64"
      },
      "outputs": [],
      "source": [
        "# basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "from IPython.display import Video, display, HTML, Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# model\n",
        "# !pip install lapx\n",
        "# !pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Active tracking\n",
        "import os\n",
        "import imageio\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero descargamos los archivos necesarios, para esto usamos una funcion de descarga desde Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 1 - Detección de objetos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos la imagen original con la que vamos a trabajar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "yVP5E6zlbPT4",
        "outputId": "b4b36271-1ad5-490d-8bce-6b8ab505adc5"
      },
      "outputs": [],
      "source": [
        "image_path = 'source/photo.png'\n",
        "Image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comenzamos con la primer deteccion de imagen, aprovechando el poder de computo local vamos a usar el mayor muestreo disponible para el modelo: 1080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "LBJmi5JEbVda",
        "outputId": "b6f2ff94-cee6-4ebb-d6e2-f383aeecd5d6"
      },
      "outputs": [],
      "source": [
        "image_save_path = 'photo-detection-1.png'\n",
        "\n",
        "results = model(image_path, imgsz=1920) # Donde transcurre todo\n",
        "for result in results:\n",
        "    result.save(image_save_path)\n",
        "\n",
        "Image(image_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos una comparacion ingresando al modelo diferentes parametros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image('source/ejemplo.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos observar que a primeras el modelo pre-entrenado funciona correctamente con sus parametros por defecto, aunque si tenemos disponibilidad computacional y podemos avanzar un poco vamos a notar diferencias. Notese la cantidad de objetos que se detectaron de mas en la segunda imagen. Esto se debe a que el modelo por defecto busca un punto de equilibrio entre rendimiento y resultados. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4NZqBnpyry"
      },
      "source": [
        "Vamos con el procesamiento de video, primero veamos el video con el que vamos a trabajar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hW7bZcxz2pF"
      },
      "outputs": [],
      "source": [
        "video_path = 'source/video.mp4'\n",
        "Video(video_path, width=1000, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se trata de un video un tanto complicado, hay demasiados objetos y muy diversos, la camara esta en constante movimiento como asi tambien la clase mayoritaria (personas), esto impone un desafio para el modelo, vamos a ver como responde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeI3OomwHYMg"
      },
      "source": [
        "Sobre la detección de objetos en video:\n",
        "\n",
        "Para la deteccion de videos vamos a utilizar **CV2** para leer el video, y **imageio** para grabar los frames con las detecciones. Esto es asi, ya que de forma nativa CV2 no tiene por defecto soporte para el codec x264. Aca tambien voy a comentar en detalle el codigo, para no repetirlo constantemente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlgVObnJ2hOl",
        "outputId": "13fb4eb5-c133-45a0-8577-bca7fc276e57"
      },
      "outputs": [],
      "source": [
        "input_path = video_path     # Ruta de entrada\n",
        "output_path = 'video-detection-1.mp4'      # Ruta de salida\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)      # Si el archivo de salida existe, que lo borre\n",
        "\n",
        "cap = cv2.VideoCapture(input_path)      # Abrimos el video\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)     # Obtenemos cuadros por segundo\n",
        "out = imageio.get_writer(output_path, fps=fps, codec='libx264')     # Abrimos el grabador de video\n",
        "\n",
        "while cap.isOpened():\n",
        "    status, frame = cap.read()      # Mientras existan cuadros\n",
        "    if not status:\n",
        "        break\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)      # Pasamos al modelo el frame en RGB\n",
        "    results = model(frame, imgsz=1920)      # Aumentamos la resolucion al modelo\n",
        "    annotated_frame = results[0].plot()     # Incorpora al frame los resultados\n",
        "    out.append_data(annotated_frame)        # Grabo el frame\n",
        "\n",
        "cap.release()       # Libero recursos de entrada\n",
        "out.close()     # Libero recursos de salida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos el video resultante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gauh-LiMVm4G"
      },
      "outputs": [],
      "source": [
        "Video(output_path, width=1000, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nuevamente, podemos apreciar que el modelo reconoce objetos bastante bien. Con estos pequeños ajustes podemos lograr detectar objetos pequeños y de dificil reconocimiento, un telefono celular, una pequeña mochila. Por otro lado vemos que el uso de CPU es absoluto, lo que asegura el uso del 100% de nuestros recursos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image('source/core.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCMkJu155c6k"
      },
      "source": [
        "# Etapa 2 - Personalización de etiquetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para este paso vamos a personalizar algunas etiquetas y colores, guardar estos valores y persistirlos durante el estadio de trabajo del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6dxEKNm4A5N"
      },
      "outputs": [],
      "source": [
        "conf_threshold = 0.6  # Valor umbral principal\n",
        "\n",
        "colors = {\n",
        "    'Automovil': (255, 0, 0),  # Rojo\n",
        "    'Persona': (0, 255, 0),    # Verde\n",
        "    'Camion': (0, 0, 255),     # Azul\n",
        "    'Colectivo': (255, 255, 0), # Amarillo\n",
        "    'Cartera': (0, 255, 255),   # Cyan\n",
        "    'Semaforo': (255, 0, 255),  # Magenta\n",
        "    'Moto': (0, 255, 255),       # Cyan\n",
        "    'Mochila': (255, 255, 255),   # Blanco\n",
        "}\n",
        "\n",
        "labels = {\n",
        "    'car': 'Automovil',\n",
        "    'person': 'Persona',\n",
        "    'truck': 'Camion',\n",
        "    'bus': 'Colectivo',\n",
        "    'handbag': 'Cartera',\n",
        "    'traffic light': 'Semaforo',\n",
        "    'motorcycle': 'Moto',\n",
        "    'backpack': 'Mochila',\n",
        "    'bench': 'Banco'\n",
        "}\n",
        "\n",
        "default_color = (0, 0, 0)  # Color por defecto para etiquetas no especificadas\n",
        "default_label = 'Desconocido'    # Etiqueta por defecto para categorías no especificadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zy6HiVuGKxW"
      },
      "source": [
        "Las clases y su significado:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "n3noc8vk3ae9",
        "outputId": "5ed571da-5170-47b2-c169-dc2d84f24180"
      },
      "outputs": [],
      "source": [
        "def detect_image(img):\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Me aseguro que sea RGB\n",
        "    results = model(img_rgb, imgsz=1920, conf=0.6) # Confianza > 60\n",
        "\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            conf = box.conf.item()\n",
        "            cls = box.cls.item() # Extraigo las clases\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0]) # Mapeo los recuadros\n",
        "            yolo_label = model.names[int(cls)]\n",
        "            label = labels.get(yolo_label, default_label) # Utilizo mis valores\n",
        "            color = colors.get(label, default_color) # Mis colores\n",
        "            confidence = conf * 100\n",
        "            text = f'{label} {confidence:.1f}%' # Informaciôn de etiqueta\n",
        "\n",
        "            # Cuadro y etiqueta en la imagen\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(img, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "    return img\n",
        "\n",
        "image_save_path = 'photo-detection-2.png'\n",
        "img_original = cv2.imread(image_path) # Me traigo la imagen\n",
        "img = detect_image(img_original) # Paso la imagen y la confianza\n",
        "\n",
        "cv2.imwrite(image_save_path, img)\n",
        "\n",
        "Image(image_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logramos los objetivos pedidos, etiquetar con nuevas etiquetas, colores y porcentaje de confianza. La observacion aqui, es que pasando este umbral, el modelo va a lo seguro, se refina. Elimina los objetos que considera ruido y se enfoca en los mejor explicados. Vamos con la etapa de video, y para tal fin vamos a reutilizar la funcion que utilizamos en la imagen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUinNK_D33-h",
        "outputId": "f99a74a6-596c-4a9a-84b9-8892323e8616"
      },
      "outputs": [],
      "source": [
        "input_path = video_path\n",
        "output_path = 'video-detection-2.mp4'\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "out = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
        "\n",
        "while cap.isOpened():\n",
        "    status, frame = cap.read()\n",
        "    if not status:\n",
        "        break\n",
        "\n",
        "    frame = detect_image(frame)  # Reutilizo la funcion de imagen\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    out.append_data(frame)  # Grabo\n",
        "\n",
        "cap.release()\n",
        "out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos el video resultante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJDbL2uMSEiy"
      },
      "outputs": [],
      "source": [
        "Video(output_path, width=1000, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nuevamente observamos los cambios en las etiquetas, los colores y los objetos detectados por ensima del threshold asignado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuaoDr0i5dCI"
      },
      "source": [
        "# Etapa 3 - Segmentación y detección de objetos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2DkWslAOox0"
      },
      "source": [
        "Para esta etapa se nos pidio contabilizar los objetos unicos en imagen y en video, para la imagen fue simple, ya que solo habia que obtener el mayor y el menor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "ella6lomxVNy",
        "outputId": "c354e93f-005e-4eb1-ef4c-5849c77784fa"
      },
      "outputs": [],
      "source": [
        "def process_image(img_path, labels, default_label='Other'):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = model(img, imgsz=1920)  # Asegúrate de que el modelo está cargado correctamente\n",
        "\n",
        "    # Contar la cantidad de objetos detectados de cada clase\n",
        "    object_counts = Counter()\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            cls = box.cls.item()\n",
        "            yolo_label = model.names[int(cls)]\n",
        "            # Aplicar etiquetas personalizadas inmediatamente\n",
        "            custom_label = labels.get(yolo_label, default_label)\n",
        "            object_counts[custom_label] += 1\n",
        "\n",
        "    if not object_counts:\n",
        "        return\n",
        "\n",
        "    # Identificar las clases con mayor y menor cantidad de objetos\n",
        "    most_common_class, least_common_class = object_counts.most_common(1)[0][0], object_counts.most_common()[-1][0]\n",
        "\n",
        "    # Procesar y resaltar solo los objetos de las clases identificadas\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            cls = box.cls.item()\n",
        "            yolo_label = model.names[int(cls)]\n",
        "            custom_label = labels.get(yolo_label, default_label)\n",
        "\n",
        "            if custom_label == most_common_class:\n",
        "                color = (255, 0, 0)  # Azul\n",
        "            elif custom_label == least_common_class:\n",
        "                color = (0, 0, 255)  # Rojo\n",
        "            else:\n",
        "                continue  # Saltar los objetos que no son de las clases identificadas\n",
        "\n",
        "            conf = box.conf.item()\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            confidence = conf * 100\n",
        "            text = f'{custom_label} {confidence:.1f}%'\n",
        "\n",
        "            # Dibujar cuadro de delimitación y texto\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(img, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "image_save_path = 'photo-detection-3.png'\n",
        "img = process_image(image_path, labels)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "cv2.imwrite(image_save_path, img)\n",
        "\n",
        "Image(image_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj7K3WgLcOvj"
      },
      "source": [
        "Los resultados obtenidos con los ajustes propuestos fueron muy buenos, siendo capaz el modelo de detectar como objeto menos frecuente la mochila que apenas se ve en la imagen. Esto, nuevamente se debe a la ampliacion de la muestra pasada al modelo.\n",
        "\n",
        "Para la parte del video, que incluye object tracking, mi primer acercamiento fue a traves de SORT, para ello tuve que modificar un repo de GIT, y hacer mil ajustes innecesarios. Por suerte leyendo la documentacion de Yolo vi que Yolov8 tenia disponible el traker de forma nativa, esta implementacion fue extremadamente mas simple que el proceso anterior, y muchos menos costosa computacionalmente.\n",
        "\n",
        "Para un video de solo unos segundos no se justifica, podrimos revisar manualmente las figuras detectadas, aunque pordriamos luego reutilizar la funcion para cualquier tipo de video y duración.\n",
        "\n",
        "Esta funcion basicamente abre el video cuadro a cuadro y genera todos los resultados en tracked_objects. Estos tienen la posicion, el id, la clase y todo lo que se le pida que guarde al modelo. Por otro lado se genera un diccionario de clases unicas, este va evaluando por id las clases y las cuenta. Posteriormente con max y min obtenemos la clase (int) que mas y menos frecuencia tiene. Vale aclarar que una persona que va del punto A al punto B se cuenta como unica persona. \n",
        "\n",
        "Veamos como funciona el seguimiento de YoloV8 por defecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g48thP2pcw5f",
        "outputId": "87711f69-4225-4cee-867f-ac5564168f28"
      },
      "outputs": [],
      "source": [
        "# input_path = video_path\n",
        "input_path = 'source/video.mp4'\n",
        "output_path = \"video-detection-3.mp4\"\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "out = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
        "total_fr = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fr = 0\n",
        "while cap.isOpened():\n",
        "    status, frame = cap.read()\n",
        "\n",
        "    if status:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Enviamos RGB para evitar salida BGR\n",
        "        results = model.track(source=frame, persist=True, imgsz=1920) # Persistiendo tracks sobre frames\n",
        "        frame = results[0].plot()\n",
        "        fr += 1\n",
        "        print(f'Frame {fr} / {total_fr}')\n",
        "        out.append_data(frame)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos los resultados obtenidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Video(output_path, width=1000, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados a primeras son muy buenos, manteniendo los ids en la mayoria de casos. Vemos que no resiste el solapamiento de personas con demasiada eficacia, aunque en terminos generales es muy aceptable.\n",
        "\n",
        "Con respecto al contador de clases unicas, decidi separar el codigo en dos, esta primera parte consta de un diccionario que almacenando las clases cuyo track_id sean unicos, esto es util para almacenar el movimiento de los objetos y contabilizarlos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP1oYsIC3joJ",
        "outputId": "62539358-b32f-47bf-bf60-06a4f9be3565"
      },
      "outputs": [],
      "source": [
        "input_path = video_path\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Yolo 8 permite procesar un video de forma directa\n",
        "tracked_objects = model.track(source=input_path, persist=True, imgsz=1920, conf=0.6)\n",
        "\n",
        "unique_class_counts = defaultdict(set)\n",
        "\n",
        "for frame in tracked_objects:\n",
        "    if hasattr(frame, 'boxes') and frame.boxes is not None:\n",
        "        boxes = frame.boxes.xyxy.cpu() # Por defecto serian tensores\n",
        "        clss = frame.boxes.cls.cpu().tolist() # Tomo las clases\n",
        "        track_ids = frame.boxes.id.int().cpu().tolist() # Tomo los track_ids\n",
        "\n",
        "        # El proposito es mantener un registro de los identificadores de seguimiento únicos para cada clase de objeto\n",
        "        for cls, track_id in zip(clss, track_ids):\n",
        "            unique_class_counts[int(cls)].add(track_id)\n",
        "\n",
        "class_counts = {cls: len(track_ids) for cls, track_ids in unique_class_counts.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos los resultados obtenidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56V7ZOUPOUOU"
      },
      "outputs": [],
      "source": [
        "class_names = {\n",
        "    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane',\n",
        "    5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
        "    10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
        "    14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
        "    20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack',\n",
        "    25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee',\n",
        "    30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite',\n",
        "    34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard',\n",
        "    38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "    43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple',\n",
        "    48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog',\n",
        "    53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch',\n",
        "    58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet',\n",
        "    62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "    67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink',\n",
        "    72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors',\n",
        "    77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'\n",
        "}\n",
        "\n",
        "# Ordenar el diccionario class_counts por el conteo de objetos de mayor a menor\n",
        "sorted_class_counts = dict(sorted(class_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Imprimir los resultados ordenados\n",
        "print('Total de objetos contabilizados:\\n')\n",
        "for cls, count in sorted_class_counts.items():\n",
        "    class_name = class_names.get(cls, 'Unknown')\n",
        "    print(f'Class {cls}: {class_name} > {count} objetos unicos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El modelo detecto 109 personas como la clase con mas elementos unicos. Para la clase con menos objetos unicos tenemos a skate, avion, camion o bus con 1 acierto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C_-HqighElrf"
      },
      "outputs": [],
      "source": [
        "# Obtener la clase con el mayor y menor número de objetos únicos\n",
        "max_class = int(max(class_counts, key=class_counts.get))\n",
        "min_class = int(min(class_counts, key=class_counts.get))\n",
        "\n",
        "# Obtener nombre real de la clase\n",
        "max_class_name = class_names.get(max_class, 'Desconocido')\n",
        "min_class_name = class_names.get(min_class, 'Desconocido')\n",
        "\n",
        "print(f'La clase única más frecuente es: {max_class_name} ({max_class}) con {class_counts[max_class]} objeto/s')\n",
        "print(f'La clase única menos frecuente es: {min_class_name} ({min_class}) con {class_counts[min_class]} objeto/s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-kJ6T5nKaPQ"
      },
      "source": [
        "Antes de pasar a la proxima etapa es valido aclarar que si bien la clase unica con mayores conteos fue persona, vamos a imponer la clase motocicleta como la menos frecuente para que se pueda apreciar en el video y no sea un objeto que aparece durante solo un frame, aunque la funcionalidad esta realizada, claro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdFxi9cyB309"
      },
      "outputs": [],
      "source": [
        "input_path = video_path\n",
        "output_path = \"video-detection-4.mp4\"\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "out = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
        "total_fr = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Para ver el progreso de avance\n",
        "fr = 0\n",
        "\n",
        "max_class_input = max_class # La resultante del código anterior\n",
        "min_class_input = 3 # Forzada para notar cambios visibles\n",
        "\n",
        "# Colores para las clases específicas\n",
        "color_map = {\n",
        "    max_class_input: (0, 0, 255),   # Azul para la clase de máxima frecuencia\n",
        "    min_class_input: (255, 0, 0)    # Rojo para la clase de mínima frecuencia\n",
        "}\n",
        "\n",
        "while cap.isOpened():\n",
        "    status, frame = cap.read()\n",
        "\n",
        "    if status:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = model.track(frame, persist=True, imgsz=1920, classes=[max_class_input, min_class_input]) # Persistiendo tracks sobre frames\n",
        "        frame_results = results[0]\n",
        "\n",
        "        for box in frame_results.boxes:     # Persistimos sobre los resultados\n",
        "            cls = int(box.cls.item())       # Obtenemos el índice de clase\n",
        "            conf = box.conf.item() * 100    # Confianza en porcentaje\n",
        "            obj_id = int(box.id.item())     # Identificador del objeto\n",
        "\n",
        "            # Convertir tensor a lista y luego mapear a enteros\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy.cpu().tolist()[0])\n",
        "            color = color_map.get(cls, (255, 255, 255))  # Color blanco por defecto\n",
        "\n",
        "            # Obtenemos el nombre de la clase a partir del índice usando model.names\n",
        "            yolo_label = model.names[cls]\n",
        "            label_name = labels.get(yolo_label, 'Desconocido')  # Nombre custom o 'Desconocido'\n",
        "\n",
        "            # Incluimos clase, ID y confianza\n",
        "            text = f'{label_name} {obj_id} - {conf:.1f}%'\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)      # Dibujamos el rectangulo\n",
        "            cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2) # Manejamos las etiquetas\n",
        "        fr += 1\n",
        "        print(f'Frame {fr} / {total_fr}')\n",
        "        out.append_data(frame)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cP9FHmMVwFj"
      },
      "outputs": [],
      "source": [
        "Video(output_path, width=1000, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1vddnMBfmj7"
      },
      "source": [
        "Conclusiones: La verdad que es muy divertido trabajar con video, el hecho de poder manipular la deteccion nos abre puertas para poder emplear esto, que no es nada dificil, en muchisimos ambitos del dia a dia. La ventaja de trabajar en entorno local fue gigante, dandome mas tiempo libre para mejorar el codigo y aumentar la potencia del modelo. Con respecto a Yolo, me parece una herramienta fundamental para el desarrollo de deteccion en imagenes, siendo su abanico de opciones inmenso y en constante desarrollo.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
